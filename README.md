# compiler

Notes: 
Written in Python 3 (PLY's) Using Lex/Yacc module 
	- Copyright (C) 2001-2018 David M. Beazley (Dabeaz LLC) All rights reserved.
	- https://github.com/dabeaz/ply 
	- http://www.dabeaz.com/ply/ply.html#ply_nn23

Usage: 
python compiler.py [testfilename] [optional T]

The optional T is for a tracking debug mode that displays what is happening within the LALR (Look ahead left to right) automation. 

#About

Compiles a modified langauge called m langauge into am stack code. Both written by my professor for the purposes of a compilers class. 
Most of the fundimentals utilized here can be applied to other compilers and languages.

Lexical Analysis (parsing part 1): 
- Lexical Analysis also known as "tokinizing" is the first stage of a compiler and generally serves to divide the input text into useful tokens.
- Here were using PLY's lex tool to divide things into tokens using regular expression matching. 
- "t_" within the code notates a token pattern (ie Regular Expression). 
- The lexar included also removes unimportant things like whitespaces and comments again using regular expressions. 
- http://www.dalkescientific.com/writings/NBN/parsing_with_ply.html is a great resource to learn more about using the PLY Lex and Yacc modules. 

Syntactical Analysis (parsing part 2): 
- Syntactical Analysis takes the token representaiton from the lexical analysis and makes sure the tokens can make up vaild expressions. 
- Again were using the PLY's lex tool but this time to follow a context-free-grammer for the given languge. 
- This stage also builds an Abstract Syntax Tree (AST) which we can use in the next stage of compilation. 
- This tree is built by building out each expression to it's full depth with all of its components and adding them to the sub tree of the component it originated from. 
- The full AST can be found under logs/ast.txt

All erros caught while parsing and lexing are caught and execution of the parser terminates after giving you a 
line number to follow up on the error. 

Semantic Analysis: 
- This is where things get interesting (and buggy)
- Here we put full expressions together and make sure the variables defined make sense in the context they are used. 
- A symbol table (ST) it generated and variables are checked against the scope they are used as well as the type they are used as. 
- An intermediate representaiton tree (IR) is created and forwarded to the next part of the program. 
- The full IR and ST of the most recent excution can be found in logs/IR.txt and log/ST.txt respectively. 

Code Optimizations: 
- This program does not perform any optimzations to the code.


Stack Code Generaton: 
- The Intermediate representaitional tree from the semantic analysis is fed in here to generate stack code. 
- Stack code is produced and written to a file inputfilename.am in the running directory
- The stack code produced is code designed for my professors am stack code runner. 


More Notes: 
- Upon excution parser.out and parsetab.py are generated by Lex/Yacc. Ignore these files or view them for pleasure but they are not "part" of the program


Things I learned: 
- Python is not a good languge to write compilers in. (But it can be done)
- Use a pattern matching languge like Haskell if your interested in compilers. 
- Lex and Yacc tools are super useful and can be taken out of the compiler context to produce some really cool things. 
- As each step in the compiler builds upon the last make sure you have the previous step nailed before moving on to the next 
- if you don't do this a ton of time will be wasted and a ton of adhoc code will be produced. ** This is your warning ** 
- Test early and test often. Catch bugs or adhoc code and wasted time will creep in. 
- Compilers are hard but super satisfying when completed. I look forward to more work in this area. 
